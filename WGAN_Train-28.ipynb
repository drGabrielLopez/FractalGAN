{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSOsVrZKp-QL"
   },
   "source": [
    "# Train a Wasserstein GANs on model on fractal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <center><img src=\"/home/ec2-user/SageMaker/FractalGAN/Space_Fractal.png\" width=100 heights=100> -->"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REFERENCES:\n",
    "    - https://towardsdatascience.com/deep-convolutional-vs-wasserstein-generative-adversarial-network-183fbcfdce1f\n",
    "    - https://github.com/keras-team/keras-io/tree/master/examples/generative\n",
    "    - https://www.researchgate.net/figure/Wasserstein-GAN-WGAN_fig1_339998063"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GOOD COURSE ON GANS:\n",
    "- https://www.coursera.org/specializations/generative-adversarial-networks-gans?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PAPERS:\n",
    "- Original WGAN: https://arxiv.org/pdf/1701.07875.pdf\n",
    "- Improved WGAN: https://arxiv.org/pdf/1704.00028.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l7tWLS7p-QU"
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GyKOxek0p-QV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL9DyXFXp-QW"
   },
   "source": [
    "## Prepare the Fractal data\n",
    "The folder contains about 60K images of size 28x28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 512\n",
    "SHAPE = (28,28)\n",
    "N = 60000\n",
    "# Size of the noise vector\n",
    "noise_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jy9ftI5sIxF6",
    "outputId": "f71b3151-8184-4df4-a6cd-3acab5b87ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import pathlib\n",
    "# data_dir = pathlib.Path('/home/ec2-user/SageMaker/FractalGAN/dataset_28/')\n",
    "data6K = image_dataset_from_directory('/home/ec2-user/SageMaker/FractalGAN/dataset_28/', batch_size=1, image_size=(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1c3lnCDwy13",
    "outputId": "a162d5dc-c2d0-4801-8337-1d33a6c980c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final dimensions =  (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Make a numpy array\n",
    "# Color channels are summed to produce a gray-scale image 255*3 = 765 \n",
    "# To normalize between [-1,1] we need to dived this number by 2:  765/2\n",
    "imgs = np.stack([ np.reshape(np.sum(dat[0].numpy(), axis=3), SHAPE).astype(\"float32\") for dat in data6K.take(N)], axis=0)\n",
    "imgs = (imgs - 382.5)/382.5\n",
    "imgs = np.expand_dims(imgs, axis=-1)\n",
    "print('final dimensions = ', imgs.shape)\n",
    "assert imgs.max() <= 1.0\n",
    "assert imgs.min() >= -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "CSvU6GtmqNs4",
    "outputId": "0fc30b6b-3da5-4190-dac9-aec9607c9059"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a Random Example\n",
    "import numpy as np\n",
    "plt.matshow(imgs[np.random.randint(0,N)][:,:,0]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input = (28,28) --resized to --> (32,32)\n",
    "- We increase the *depth* size in powers of 2: 64 -> 128 \n",
    "- We decrease the output *width* in powers of 2\n",
    "- When reaching a 2*2 output array we do a FC network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yPrN5BQp-QY",
    "outputId": "88aa2dbc-2aed-40a0-fc08-22f7c4271ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 4,305,409\n",
      "Trainable params: 4,305,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)  # the output is a linear function\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTvgmwGUp-QZ"
   },
   "source": [
    "# WGAN - Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as the dicriminator but inj \"reverse mode\"\n",
    "- Use UpSample+Conv2D instead of Deconvolution (avoids stridded patterns)\n",
    "- Grow laterally the output in powers of 2\n",
    "- Decrease the filter *depth* in powers of 2\n",
    "- Use leakReLu ==> Allows learning even beyond the \"reject zone\" (still acts as a logical gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thzhW4yOp-Qh",
    "outputId": "93d842df-9dcf-43c2-fc1f-4d84390715ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 16, 16, 64)        73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 1)         576       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 1)         4         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "cropping2d (Cropping2D)      (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 910,660\n",
      "Trainable params: 902,082\n",
      "Non-trainable params: 8,578\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "    x = layers.Cropping2D((2, 2))(x)\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_lmAMp0p-Q2"
   },
   "source": [
    "## Create the WGAN-GP model\n",
    "\n",
    "We need to override the `train_step` for training. References: \n",
    "- https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\n",
    "- https://github.com/keras-team/keras/blob/master/keras/engine/training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n8RQg-aKp-Q3"
   },
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEuPnepBp-Q4"
   },
   "source": [
    "## Save images generated during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s0VRwe8bp-Q8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.preprocessing.image.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP18hmgCp-RC"
   },
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qg4L3nsJp-RG",
    "outputId": "79bd4c7f-2955-4acc-afac-d09f492d370f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 40s 338ms/step - d_loss: -7.5656 - g_loss: -9.1668\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 40s 339ms/step - d_loss: -4.7882 - g_loss: -6.8787\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 40s 340ms/step - d_loss: -3.8738 - g_loss: -4.0449\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 40s 341ms/step - d_loss: -3.2255 - g_loss: -3.1691\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 40s 343ms/step - d_loss: -2.7644 - g_loss: -3.6727\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -2.4424 - g_loss: -2.4806\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -2.1262 - g_loss: -1.2670\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -1.9474 - g_loss: -2.8778\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -1.7679 - g_loss: -2.7305\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 41s 345ms/step - d_loss: -1.5204 - g_loss: -2.2993\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.5297 - g_loss: -0.8425\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 40s 343ms/step - d_loss: -1.4691 - g_loss: 1.2383\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.3583 - g_loss: -0.7893\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -1.2889 - g_loss: -0.3901\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.2211 - g_loss: -1.9732\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -1.2274 - g_loss: -1.7657\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.2393 - g_loss: -3.0455\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.1830 - g_loss: -2.8854\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 41s 343ms/step - d_loss: -1.1693 - g_loss: -3.0942\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 41s 344ms/step - d_loss: -1.1646 - g_loss: -2.9080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2e74229ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "# if the discriminator does a good job this loss will be zero because D(G(x))=0\n",
    "# the loss of the generator triest to push that away \n",
    "# negative sign in the loss = go away from zero\n",
    "# positive sign in the loss = go towards zero\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
    "\n",
    "# Instantiate the WGAN model.\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the WGAN model.\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training the model.\n",
    "wgan.fit(imgs, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TjxoEcop-RG"
   },
   "source": [
    "# Display the last generated images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "YdkbfOFWp-RH",
    "outputId": "6ebccace-5d48-40be-f18c-bd5ead83852a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2cdc2e64e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "i = np.random.randint(0,19)\n",
    "img = mpimg.imread(f'generated_img_0_{i}.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WGAN_gabs_1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "gabs",
   "language": "python",
   "name": "gabs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
